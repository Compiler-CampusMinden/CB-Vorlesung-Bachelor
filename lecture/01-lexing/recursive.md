# Lexer: Handcodierte Implementierung

> [!IMPORTANT]
>
> <details open>
>
> <summary><strong>ğŸ¯ TL;DR</strong></summary>
>
> <picture><source media="(prefers-color-scheme: light)" srcset="images/architektur_cb_lexer_light.png"><source media="(prefers-color-scheme: dark)" srcset="images/architektur_cb_lexer_dark.png"><img src="images/architektur_cb_lexer.png"></picture>
>
> Der Lexer (auch â€œScannerâ€) soll den Zeichenstrom in eine Folge von
> Token zerlegen. Zur Spezifikation der Token werden regulÃ¤re AusdrÃ¼cke
> verwendet.
>
> Von Hand implementierte Lexer arbeiten Ã¼blicherweise rekursiv und
> verarbeiten immer das nÃ¤chste Zeichen im Eingabestrom. Die
> Arbeitsweise erinnert an LL-Parser (vgl.
> [LL-Parser](../02-parsing/ll-parser-impl.md)).
>
> Lexer mÃ¼ssen sehr effizient sein, da sie noch direkt auf der
> niedrigsten Abstraktionsstufe arbeiten und u.U. oft durchlaufen
> werden. Deshalb setzt man hier gern spezielle Techniken wie Puffern
> von Zeichen Ã¼ber einen Doppel-Puffer ein.
>
> Die Palette an Fehlerbehandlungsstrategien im Lexer reichen von
> â€œaufgebenâ€ Ã¼ber den â€œPanic Modeâ€ (â€œgobbelnâ€ von Zeichen, bis wieder
> eines passt) und Ein-Schritt-Transformationen bis hin zu speziellen
> Lexer-Regeln, die beispielsweise besonders hÃ¤ufige Typos abfangen.
> </details>

> [!TIP]
>
> <details open>
>
> <summary><strong>ğŸ¦ Videos</strong></summary>
>
> - [VL Handcodierte Lexer](https://youtu.be/N0WJQ4UkXkM)
>
> </details>

## Lexer: Erzeugen eines Token-Stroms aus einem Zeichenstrom

Aus dem Eingabe(-quell-)text

``` c
/* demo */
a= [5  , 6]     ;
```

erstellt der Lexer (oder auch Scanner genannt) eine Sequenz von Token:

    <ID, "a"> <ASSIGN> <LBRACK> <NUM, 5> <COMMA> <NUM, 6> <RBRACK> <SEMICOL>

## Manuelle Implementierung: Rekursiver Abstieg

``` python
def nextToken():
    while (peek != EOF):  # globale Variable, Ã¼ber consume()
        switch (peek):
            case ' ': case '\t': case '\n': WS(); continue
            case '[': consume(); return Token(LBRACK, '[')
            ...
            default:
                if isLetter(peek): return NAME()
                raise Error("invalid character: "+peek)
    return Token(EOF_Type, "<EOF>")

def WS():
    while (peek == ' ' || peek == '\t' || ...): consume()

def NAME():
    buf = StringBuilder()
    do { buf.append(peek); consume(); } while (isLetter(peek))
    return Token(NAME, buf.toString())
```

Die manuelle Implementierung â€œdenktâ€ nicht in den ZustÃ¤nden des DFA,
sondern orientiert sich immer am aktuellen Zeichen â€œ`peek`â€. AbhÃ¤ngig
von dessen AusprÃ¤gung wird entweder direkt ein Token erzeugt und das
Zeichen aus dem Eingabestrom entfernt sowie das nÃ¤chste Zeichen
eingelesen (mittels der Funktion `consume()`, nicht dargestellt im
Beispiel), oder man ruft weitere Funktionen auf, die das GewÃ¼nschte
erledigen, beispielsweise um White-Spaces zu entfernen oder um einen
Namen einzulesen: Nach einem Buchstaben werden alle folgenden Buchstaben
dem Namen (Bezeichner) hinzugefÃ¼gt. Sobald ein anderes Zeichen im
Eingabestrom erscheint, wird das Namen-Token erzeugt.

Die Funktion `consume()` â€œverbrauchtâ€ das aktuelle Zeichen â€œ`peek`â€ und
holt das nÃ¤chste Zeichen aus dem Eingabestrom.

*Anmerkung*: HÃ¤ufig findet man im Lexer keinen â€œschÃ¶nenâ€
objektorientierten Ansatz. Dies ist i.d.R. GeschwindigkeitsgrÃ¼nden
geschuldet â€¦

## Read-Ahead: Unterscheiden von â€œ*\<*â€ und â€œ*\<=*â€

``` python
def nextToken():
    while (peek != EOF):  # globale Variable
        switch (peek):
            case '<':
                if match('='): consume(); return Token(LE, "<=")
                else: consume(); return Token(LESS, '<')
            ...
    return Token(EOF_Type, "<EOF>")

def match(c):   # Lookahead: Ein Zeichen
    consume()
    if (peek == c): return True
    else: rollBack(); return False
```

Um die Token â€œ`<`â€ und â€œ`<=`â€ unterscheiden zu kÃ¶nnen, mÃ¼ssen wir ein
Zeichen vorausschauen: Wenn nach dem â€œ`<`â€ noch ein â€œ`=`â€ kommt, ist es
â€œ`<=`â€, sonst â€œ`<`â€.

Erinnerung: Die Funktion `consume()` liest das nÃ¤chste Zeichen aus dem
Eingabestrom und speichert den Wert in der globalen Variable `peek`.

FÃ¼r das Read-Ahead wird die Funktion `match()` definiert, die zunÃ¤chst
das bereits bekannte Zeichen, in diesem Fall das â€œ`<`â€ durch das nÃ¤chste
Zeichen im Eingabestrom ersetzt (Aufruf von `consume()`). Falls der
Vergleich des Lookahead-Zeichens mit dem gesuchten Zeichen erfolgreich
ist, liegt das â€œgrÃ¶ÃŸereâ€ Token vor, also â€œ`<=`â€. Dann wird noch das
â€œ`=`â€ durch das nÃ¤chste Zeichen ersetzt und das Token `LE` gebildet.
Anderenfalls muss das zuviel gelesene Zeichen wieder in den Eingabestrom
zurÃ¼ckgelegt werden (`rollBack()`).

## Puffern des Input-Stroms: Double Buffering

Das Einlesen einzelner Zeichen fÃ¼hrt zwar zu eleganten algorithmischen
LÃ¶sungen, ist aber zur Laufzeit deutlich â€œteurerâ€ als das Einlesen mit
gepufferten I/O-Operationen, die eine ganze Folge von Zeichen einlesen
(typischerweise einen ganzen Disk-Block, beispielsweise 4096 Zeichen).

Dazu kann man einen Ringpuffer nutzen, den man mit Hilfe von zwei gleich
groÃŸen `char`-Puffern mit jeweils der LÃ¤nge $`N`$ simulieren kann.
($`N`$ sollte dann der LÃ¤nge eines Disk-Blocks entsprechen.)

Vergleiche auch [Wikipedia: â€œCircular
Bufferâ€](https://en.wikipedia.org/wiki/Circular_buffer).

<picture><source media="(prefers-color-scheme: light)" srcset="images/doublebuffer_light.png"><source media="(prefers-color-scheme: dark)" srcset="images/doublebuffer_dark.png"><img src="images/doublebuffer.png" width="65%"></picture>

``` python
start = 0; end = 0; fill(buffer[0:n])

def consume():
    peek = buffer[start]
    start = (start+1) mod 2n
    if (start mod n == 0):
        fill(buffer[start:start+n-1])
        end = (start+n) mod 2n

def rollBack():
    if (start == end): raise Error("roll back error")
    start = (start-1) mod 2n
```

ZunÃ¤chst wird nur der vordere Pufferteil durch einen passenden
Systemaufruf gefÃ¼llt.

Beim Weiterschalten im simulierten DFA oder im manuell kodierten Lexer
(Funktionsaufruf von `consume()`) wird das nÃ¤chste Zeichen aus dem
vorderen Pufferteil zurÃ¼ckgeliefert. Ãœber die Modulo-Operation bleibt
der Pointer `start` immer im Speicherbereich der beiden Puffer.

Wenn man das Ende des vorderen Puffers erreicht, wird der hintere Puffer
mit einem Systemaufruf gefÃ¼llt. Gleichzeitig wird ein Hilfspointer `end`
auf den Anfang des vorderen Puffers gesetzt, um Fehler beim Roll-Back zu
erkennen.

Wenn man das Ende des hinteren Puffers erreicht, wird der vordere Puffer
nachgeladen und der Hilfspointer auf den Anfang des hinteren Puffers
gesetzt.

Im Grunde ist also immer ein Puffer der â€œArbeitspufferâ€ und der andere
enthÃ¤lt die bereits gelesene (verarbeitete) Zeichenkette. Wenn beim
Nachladen weniger als $`N`$ Zeichen gelesen werden, liefert der
Systemaufruf als letztes â€œZeichenâ€ ein `EOF`. Beim Verarbeiten wird
`peek` entsprechend diesen Wert bekommen und der Lexer muss diesen Wert
abfragen und berÃ¼cksichtigen.

FÃ¼r das Roll-Back wird der `start`-Pointer einfach dekrementiert (und
mit einer Modulo-Operation auf den Speicherbereich der beiden Puffer
begrenzt). Falls dabei der `end`-Pointer â€œeingeholtâ€ wird, ist der
`start`-Pointer durch beide Puffer zurÃ¼ckgelaufen und es gibt keinen
frÃ¼heren Input mehr. In diesem Fall wird entsprechend ein Fehler
gemeldet.

*Anmerkung*: In der Regel sind die Lexeme kurz und man muss man nur ein
bis zwei Zeichen im Voraus lesen. Dann ist eine PuffergrÃ¶ÃŸe von 4096
Zeichen mehr als ausreichend groÃŸ und man sollte nicht in Probleme
laufen. Wenn der nÃ¶tige Look-Ahead aber beliebig groÃŸ werden kann, etwa
bei Sprachen ohne reservierte SchlÃ¼sselwÃ¶rtern oder bei
Kontext-sensitiven Lexer-Grammatiken (denken Sie etwa an die
EinrÃ¼cktiefe bei Python), muss man andere Strategien verwenden. ANTLR
beispielsweise vergrÃ¶ÃŸert in diesem Fall den Puffer dynamisch,
alternativ kÃ¶nnte man die AuflÃ¶sung zwischen SchlÃ¼sselwÃ¶rtern und
Bezeichnern dem Parser Ã¼berlassen.

## Typische Muster fÃ¼r Erstellung von Token

1.  SchlÃ¼sselwÃ¶rter

    - Ein eigenes Token (RE/DFA) fÃ¼r jedes SchlÃ¼sselwort, oder
    - Erkennung als Name (`ID`) und nachtrÃ¤glich Vergleich mit
      WÃ¶rterbuch sowie Korrektur des Tokentyps

    Wenn SchlÃ¼sselwÃ¶rter Ã¼ber je ein eigenes Token abgebildet werden,
    benÃ¶tigt man fÃ¼r jedes SchlÃ¼sselwort einen eigenen RE bzw. DFA. Die
    Erkennung als Bezeichner und das Nachschlagen in einem WÃ¶rterbuch
    (geeignete Hashtabelle) sowie die entsprechende nachtrÃ¤gliche
    Korrektur des Tokentyps kann die Anzahl der ZustÃ¤nde im Lexer
    signifikant reduzieren!

2.  Operatoren

    - Ein eigenes Token fÃ¼r jeden Operator, oder
    - Gemeinsames Token fÃ¼r jede Operatoren-Klasse

3.  Bezeichner: Ein gemeinsames Token fÃ¼r alle Namen

4.  Zahlen: Ein gemeinsames Token fÃ¼r alle numerischen Konstante (ggf.
    Integer und Float unterscheiden)

    FÃ¼r Zahlen fÃ¼hrt man oft ein Token â€œ`NUM`â€ ein. Als Attribut
    speichert man das Lexem i.d.R. als String. Alternativ kann man
    (zusÃ¤tzlich) das Lexem in eine Zahl konvertieren und als
    (zusÃ¤tzliches) Attribut speichern. Dies kann in spÃ¤teren Stufen viel
    Arbeit sparen.

5.  String-Literale: Ein gemeinsames Token

6.  Komma, Semikolon, Klammern, â€¦: Je ein eigenes Token

7.  Regeln fÃ¼r White-Space und Kommentare etc. â€¦

    Normalerweise benÃ¶tigt man Kommentare und White-Spaces in den
    folgenden Stufen nicht und entfernt diese deshalb aus dem
    Eingabestrom. Dabei kÃ¶nnte man etwa White-Spaces in den Pattern der
    restlichen Token berÃ¼cksichtigen, was die Pattern aber sehr komplex
    macht. Die Alternative sind zusÃ¤tzliche Pattern, die auf die
    White-Space und anderen nicht benÃ¶tigten Inhalt matchen und diesen
    â€œgerÃ¤uschlosâ€ entfernen. Mit diesen Pattern werden keine Token
    erzeugt, d.h. der Parser und die anderen Stufen bemerken nichts von
    diesem Inhalt.

    Gelegentlich benÃ¶tigt man aber auch Informationen Ã¼ber White-Spaces,
    beispielsweise in Python. Dann mÃ¼ssen diese Token wie normale Token
    an den Parser weitergereicht werden.

Jedes Token hat i.d.R. ein Attribut, in dem das Lexem gespeichert wird.
Bei eindeutigen Token (etwa bei eigenen Token je SchlÃ¼sselwort oder bei
den Interpunktions-Token) kann man sich das Attribut auch sparen, da das
Lexem durch den Tokennamen eindeutig rekonstruierbar ist.

| Token | Beschreibung | Beispiel-Lexeme |
|:---|:---|:---|
| `if` | Zeichen `i` und `f` | `if` |
| `relop` | `<` oder `>` oder `<=` oder `>=` oder `==` oder `!=` | `<`, `<=` |
| `id` | Buchstabe, gefolgt von Buchstaben oder Ziffern | `pi`, `count`, `x3` |
| `num` | Numerische Konstante | `42`, `3.14159`, `0` |
| `literal` | Alle Zeichen auÃŸer `"`, in `"` eingeschlossen | `"core dumped"` |

*Anmerkung*: Wenn es mehrere matchende REs gibt, wird in der Regel das
lÃ¤ngste Lexem bevorzugt. Wenn es mehrere gleich lange Alternativen gibt,
muss man mit Vorrangregeln bzgl. der Token arbeiten.

## Fehler bei der Lexikalischen Analyse

Problem: Eingabestrom sieht so aus: `fi (a==42) { ... }`

Der Lexer kann nicht erkennen, ob es sich bei `fi` um ein vertipptes
SchlÃ¼sselwort handelt oder um einen Bezeichner: Es kÃ¶nnte sich um einen
Funktionsaufruf der Funktion `fi()` handeln â€¦ Dieses Problem kann erst
in der nÃ¤chsten Stufe sinnvoll erkannt und behoben werden.

=\> Was tun, wenn keines der Pattern auf den Anfang des Eingabestroms
passt?

Optionen:

- Aufgeben â€¦

  Eventuell vielleicht sogar die beste und einfachste Variante :-)

<!-- -->

- â€œPanic Modeâ€: Entferne so lange Zeichen, bis ein Pattern passt.

  Das verwirrt u.U. den Parser, kann aber insbesondere in interaktiven
  Umgebungen hilfreich sein. Ggf. kann man dem Parser auch
  signalisieren, dass hier ein Problem vorlag.

<!-- -->

- Ein-Schritt-Transformationen:
  - FÃ¼ge fehlendes Zeichen in Eingabestrom ein.
  - Entferne ein Zeichen aus Eingabestrom.
  - Vertausche ein Zeichen:
    - Ersetze ein Zeichen durch ein anderes.
    - Vertausche zwei benachbarte Zeichen.

  Diese Transformationen versuchen, den Input in einem Schritt zu
  reparieren. Das ist durchaus sinnvoll, da in der Praxis die meisten
  Fehler in dieser Stufe durch ein einzelnes Zeichen hervorgerufen
  werden: Es fehlt ein Zeichen oder es ist eines zuviel im Input. Es
  liegt ein falsches Zeichen vor (Tippfehler) oder zwei benachbarte
  Zeichen wurden verdreht â€¦

  Im Prinzip kÃ¶nnte man auch eine allgemeinere Strategie versuchen,
  indem man diejenige Transformation mit der *kleinsten Anzahl von
  Schritten* zur Fehlerbehebung bestimmt. Beispiele dafÃ¼r finden sich im
  Bereich Natural Language Processing (*NLP*), etwa die
  Levenshtein-Distanz oder der SoundEx-Algorithmus oder sogar
  Hidden-Markov-Modelle. Allerdings muss man sich in Erinnerung rufen,
  dass gerade in dieser ersten Phase eines Compilers die Geschwindigkeit
  stark im Fokus steht und eine ausgefeilte Fehlerkorrekturstrategie die
  vielen kleinen Optimierungen schnell wieder zunichte machen kann.

<!-- -->

- Fehler-Regeln: Matche typische Typos

  Gelegentlich findet man in den Grammatiken fÃ¼r den Lexer extra Regeln,
  die hÃ¤ufige bzw. typische Typos matchen und dann passend darauf
  reagieren.

## Wrap-Up

- Zusammenhang DFA, RE und Lexer

<!-- -->

- Implementierungsansatz: Manuell codiert (rekursiver Abstieg)

<!-- -->

- Read-Ahead
- Puffern mit Doppel-Puffer-Strategie

<!-- -->

- Typische Fehler beim Scannen

## ğŸ“– Zum Nachlesen

- Nystrom ([2021](#ref-Nystrom2021)): Kapitel 4
- Parr ([2010](#ref-Parr2010)): Kapitel 2 (â€œPattern 2: LL(1)
  Recursive-Decent Lexerâ€)

> [!NOTE]
>
> <details>
>
> <summary><strong>âœ… Lernziele</strong></summary>
>
> - k1: Ich kenne die Aufgaben eines Lexers
> - k2: Ich kann die manuelle Implementierung eines Lexers mit Hilfe von
>   rekursivem Abstieg erklÃ¤ren
> - k2: Ich kann den Umgang mit dem Doppel-Puffer erklÃ¤ren
> - k2: Ich kann die Varianten bei der Erkennung von SchlÃ¼sselwÃ¶rtern an
>   einem Beispiel verdeutlichen
> - k2: Ich kann typische Fehler und LÃ¶sungsansÃ¤tze in der lexikalischen
>   Analyse erklÃ¤ren
> - k3: Ich kann fÃ¼r ein Problem eine typische Einteilung von Token
>   vornehmen
> - k3: Ich kann einen Top-Down-Lexer mit Read-Ahead und intelligenter
>   Pufferung implementieren
>
> </details>

> [!TIP]
>
> <details>
>
> <summary><strong>ğŸ… Challenges</strong></summary>
>
> **Manuell implementierter Lexer**
>
> Betrachten Sie die folgende einfache Sprache:
>
>     a = 10 - 5     # Zuweisung des Ausdruckes 10-5 (Integer-Wert 5) an Variable a
>     b = a + 2 * 3  # Zuweisung von 16 an Variable b
>     c = a != b     # Zuweisung eines boolschen Werts an c
>
> Es gibt nur Statements und Expressions:
>
> - Statement: Zuweisung; jedes Statement endet mit einem NL
> - Expression: Zahl, Variable, Addition, Subtraktion, Multiplikation
>   (mit Ã¼blichem Vorrang), Vergleich
>
> **Aufgaben**:
>
> - Geben Sie eine ANTLR-Grammatik fÃ¼r diese Sprache an.
> - Implementieren Sie analog zum Vorgehen in der Vorlesung einen Lexer
>   fÃ¼r diese Sprache. (Nur den Lexer, den Parser besprechen wir in
>   einer anderen [Sitzung](../02-parsing/ll-parser-impl.md).)
>
> </details>

------------------------------------------------------------------------

> [!NOTE]
>
> <details>
>
> <summary><strong>ğŸ‘€ Quellen</strong></summary>
>
> <div id="refs" class="references csl-bib-body hanging-indent">
>
> <div id="ref-Nystrom2021" class="csl-entry">
>
> Nystrom, R. 2021. *Crafting Interpreters*. Genever Benning.
> <https://github.com/munificent/craftinginterpreters>.
>
> </div>
>
> <div id="ref-Parr2010" class="csl-entry">
>
> Parr, T. 2010. *Language Implementation Patterns*. Pragmatic
> Bookshelf.
> <https://learning.oreilly.com/library/view/language-implementation-patterns/9781680500097/>.
>
> </div>
>
> </div>
>
> </details>

------------------------------------------------------------------------

<img src="https://licensebuttons.net/l/by-sa/4.0/88x31.png" width="10%">

Unless otherwise noted, this work is licensed under CC BY-SA 4.0.

<blockquote><p><sup><sub><strong>Last modified:</strong> ce96fcd (lecture: fix readings (LL Lexer), 2025-10-20)<br></sub></sup></p></blockquote>
